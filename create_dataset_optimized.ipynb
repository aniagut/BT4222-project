{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iGC00_sBukw",
        "outputId": "be4120d8-2c3b-4bab-8214-d813a4287ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "loading dataset\n",
            "/content\n",
            "Merging behaviour and articles\n",
            "Merging user and read_time\n",
            "User same/diff features\n",
            "Origin same/diff features\n",
            "origin features\n",
            "\n",
            "Training data time range: 2023-05-18 07:00:01 to 2023-05-23 06:59:59\n",
            "Validation data time range: 2023-05-23 07:00:02 to 2023-05-24 06:59:54\n",
            "Test data time range: 2023-05-24 07:00:04 to 2023-05-25 06:59:58\n",
            "saving to parquet\n",
            "Saved train_dataset_ebnerd_small.parquet of length: 1882518\n",
            "Saved val_dataset_ebnerd_small.parquet of length: 343776\n",
            "Saved test_dataset_ebnerd_small.parquet of length: 359446\n",
            "Took: 95.39 seconds.\n",
            "Training data size: 1,882,518 samples\n",
            "Validation data size: 343,776 samples\n",
            "Test data size: 359,446 samples\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "start = time.time()\n",
        "# Paths and configuration\n",
        "print(\"loading dataset\")\n",
        "dataset_type = \"ebnerd_small\"\n",
        "print(os.getcwd())\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "# validation_path = os.path.join(base_path, \"validation\")\n",
        "behaviors_path = os.path.join(base_path, \"behaviors.parquet\")\n",
        "# behaviors_path = os.path.join(validation_path, \"behaviors.parquet\")\n",
        "history_path = os.path.join(base_path, \"history.parquet\")\n",
        "# history_path = os.path.join(validation_path, \"history.parquet\")\n",
        "articles_path = os.path.join(base_path, \"articles.parquet\")\n",
        "\n",
        "embeddings_path = os.path.join(base_path, \"article_embeddings.parquet\")\n",
        "clustering_path = os.path.join(base_path, \"clustering_results.parquet\")\n",
        "\n",
        "# Column selections\n",
        "articles_columns = [\"article_id\",\n",
        "                    \"premium\", \"category\",\n",
        "                    \"subcategory\", \"sentiment_score\",\n",
        "                    \"sentiment_label\", \"published_time\"]\n",
        "\n",
        "behaviors_columns = [\"impression_id\",\n",
        "                     \"device_type\", \"article_ids_inview\",\n",
        "                     \"article_ids_clicked\",\n",
        "                     \"user_id\", \"is_sso_user\",\n",
        "                     \"is_subscriber\", \"session_id\",\n",
        "                     # origin article features:\n",
        "                     \"article_id\", \"impression_time\",\n",
        "                     \"read_time\", \"scroll_percentage\"]\n",
        "\n",
        "# Load data\n",
        "behaviors = pd.read_parquet(behaviors_path)[behaviors_columns]\n",
        "behaviors.rename(columns={\"article_id\": \"origin_article_id\",\n",
        "                          \"read_time\": \"origin_read_time\",\n",
        "                          \"scroll_percentage\": \"origin_scroll_percentage\"}, inplace=True)\n",
        "\n",
        "history = pd.read_parquet(history_path)\n",
        "articles = pd.read_parquet(articles_path)[articles_columns]\n",
        "embeddings_df = pd.read_parquet(embeddings_path)\n",
        "embedding_clusters = pd.read_parquet(clustering_path)\n",
        "articles = articles.merge(embedding_clusters, left_on=\"article_id\", right_on=\"article_id\", how= \"left\")\n",
        "\n",
        "############################\n",
        "# CATEGORICAL VARIABLES\n",
        "############################\n",
        "\n",
        "# Join Behaviors on articles and get the genre o\n",
        "behaviors_articles = behaviors.merge(articles,\n",
        "                                     left_on=\"origin_article_id\",\n",
        "                                     right_on=\"article_id\", how=\"left\")\n",
        "\n",
        "# Create origin features and fill nan with neutral values for home page\n",
        "homepage_category = 0\n",
        "assert homepage_category not in behaviors_articles[\"category\"].values\n",
        "homepage_id = 0\n",
        "assert homepage_id not in behaviors_articles[\"origin_article_id\"].values\n",
        "behaviors[\"origin_article_id\"] = behaviors[\"origin_article_id\"].fillna(homepage_id)\n",
        "homepage_cluster = np.nanmax(behaviors_articles[\"cluster\"].values)+1\n",
        "assert homepage_cluster not in behaviors_articles[\"cluster\"].values\n",
        "\n",
        "# Features\n",
        "behaviors[\"coming_from_home_page\"] = behaviors[\"origin_article_id\"] == homepage_id\n",
        "behaviors[\"origin_cluster\"] = behaviors_articles[\"cluster\"].fillna(homepage_cluster)\n",
        "behaviors[\"origin_category\"] = behaviors_articles[\"category\"].fillna(homepage_category)\n",
        "behaviors[\"origin_scroll_percentage\"] = behaviors_articles[\"origin_scroll_percentage\"].fillna(0)\n",
        "behaviors[\"origin_sentiment_label\"] = behaviors_articles[\"sentiment_label\"].fillna(\"Neutral\")\n",
        "behaviors[\"origin_sentiment_score\"] = behaviors_articles[\"sentiment_score\"].fillna(0.5)\n",
        "behaviors[\"origin_published_time\"] = behaviors_articles[\"published_time\"].fillna(behaviors[\"impression_time\"])\n",
        "\n",
        "# WE FILL WITH THE TIME OF THE IMPRESSION, AS THE FRONT PAGE IS ALWAYS UPDATED\n",
        "\n",
        "\n",
        "# Explode arrays in `history` to get individual article impressions\n",
        "history_exploded = history.explode(['article_id_fixed',\n",
        "                                    'impression_time_fixed',\n",
        "                                    'scroll_percentage_fixed',\n",
        "                                    'read_time_fixed'])\n",
        "\n",
        "# Rename columns for clarity\n",
        "history_exploded = history_exploded.rename(columns={\n",
        "    \"article_id_fixed\": \"article_id\",\n",
        "    \"impression_time_fixed\": \"impression_time\",\n",
        "    \"scroll_percentage_fixed\": \"scroll_percentage\",\n",
        "    \"read_time_fixed\": \"read_time\"\n",
        "})\n",
        "\n",
        "# Join with articles dataset to get additional features\n",
        "history_exploded = history_exploded.merge(\n",
        "    articles[['article_id', 'sentiment_label', 'category']],\n",
        "    on='article_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Calculate user-level metrics\n",
        "user_read_time_avg = history_exploded.groupby('user_id')['read_time'].mean().reset_index(name='user_average_read_time')\n",
        "user_scroll_avg = history_exploded.groupby('user_id')['scroll_percentage'].mean().reset_index(\n",
        "    name='user_average_scroll_percentage')\n",
        "\n",
        "# Merge user-level metrics into exploded history\n",
        "history_exploded = history_exploded.merge(user_read_time_avg, on='user_id', how='left')\n",
        "history_exploded = history_exploded.merge(user_scroll_avg, on='user_id', how='left')\n",
        "\n",
        "\n",
        "# Define function to calculate impression frequency (average time between consecutive impressions)\n",
        "def calculate_user_impression_frequency(impression_times):\n",
        "    if len(impression_times) < 2:\n",
        "        return 0\n",
        "    time_diffs = np.diff(impression_times).astype('timedelta64[s]')\n",
        "    return np.mean(time_diffs)\n",
        "\n",
        "\n",
        "# Apply impression frequency calculation per user\n",
        "history_exploded[\"user_impression_frequency\"] = history_exploded.groupby('user_id')['impression_time'].transform(\n",
        "    lambda x: calculate_user_impression_frequency(x.values) if x.count() > 1 else 0\n",
        ")\n",
        "history_exploded[\"user_impression_frequency\"] = history_exploded[\"user_impression_frequency\"].dt.total_seconds()\n",
        "\n",
        "# Calculate favorite and least favorite categories per user\n",
        "category_counts = history_exploded.groupby(['user_id', 'category']).size().reset_index(name='count')\n",
        "\n",
        "# Favorite category\n",
        "favorite_category = category_counts.loc[category_counts.groupby('user_id')['count'].idxmax()]\n",
        "history_exploded = history_exploded.merge(\n",
        "    favorite_category[['user_id', 'category']],\n",
        "    on='user_id',\n",
        "    how='left',\n",
        "    suffixes=('', '_favorite')\n",
        ").rename(columns={\"category_favorite\": \"favorite_category\"})\n",
        "\n",
        "# Least favorite category\n",
        "least_favorite_category = category_counts.loc[category_counts.groupby('user_id')['count'].idxmin()]\n",
        "history_exploded = history_exploded.merge(\n",
        "    least_favorite_category[['user_id', 'category']],\n",
        "    on='user_id',\n",
        "    how='left',\n",
        "    suffixes=('', '_least_favorite')\n",
        ").rename(columns={\"category_least_favorite\": \"least_favorite_category\"})\n",
        "\n",
        "# Calculate interaction score\n",
        "history_exploded['user_interaction_score'] = (\n",
        "                                                     history_exploded['user_average_read_time'] + history_exploded[\n",
        "                                                 'user_average_scroll_percentage']\n",
        "                                             ) / 2\n",
        "\n",
        "# Calculate the dominant sentiment label for each user\n",
        "dominant_mood = history_exploded.groupby('user_id')['sentiment_label'].agg(\n",
        "    lambda x: x.value_counts().idxmax()).reset_index(name='user_mood')\n",
        "\n",
        "# Merge user mood into exploded history\n",
        "history_exploded = history_exploded.merge(dominant_mood, on='user_id', how='left')\n",
        "\n",
        "# Merge features back into the original history DataFrame to get one value per user\n",
        "history_FE = history_exploded.groupby('user_id').agg({\n",
        "    'user_average_read_time': 'mean',\n",
        "    'user_average_scroll_percentage': 'mean',\n",
        "    'user_impression_frequency': 'mean',\n",
        "    'favorite_category': 'first',\n",
        "    'least_favorite_category': 'first',\n",
        "    'user_interaction_score': 'mean',\n",
        "    'user_mood': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "############################\n",
        "# SPLITTING AND MERGING\n",
        "############################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Pre-process behaviors for merging\n",
        "behaviors = behaviors.explode(\"article_ids_inview\").reset_index(drop=True)\n",
        "behaviors[\"article_ids_clicked\"] = behaviors[\"article_ids_clicked\"].apply(\n",
        "     lambda x: int(x[0]) if isinstance(x, (list, np.ndarray)) and len(x) > 0 else np.nan\n",
        ")\n",
        "behaviors[\"clicked\"] = behaviors.apply(\n",
        "    lambda x: int(x[\"article_ids_inview\"]) == x[\"article_ids_clicked\"] if pd.notna(x[\"article_ids_clicked\"]) else False,\n",
        "    axis=1\n",
        ")\n",
        "print(\"Merging behaviour and articles\")\n",
        "# Merge data\n",
        "# 1. Merge behaviors and articles\n",
        "merged_data = pd.merge(\n",
        "    behaviors,\n",
        "    articles,\n",
        "    left_on=\"article_ids_inview\",\n",
        "    right_on=\"article_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"Merging user and read_time\")\n",
        "# 2. Merge user read time\n",
        "merged_data = pd.merge(\n",
        "    merged_data,\n",
        "    history_FE,\n",
        "    left_on=\"user_id\",\n",
        "    right_on=\"user_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"User same/diff features\")\n",
        "merged_data[\"user_article_same_mood\"] = merged_data[\"sentiment_label\"] == merged_data[\"user_mood\"]\n",
        "merged_data[\"user_article_favorite\"] = merged_data[\"category\"] == merged_data[\"favorite_category\"]\n",
        "merged_data[\"user_article_least_favorite\"] = merged_data[\"category\"] == merged_data[\"least_favorite_category\"]\n",
        "\n",
        "print(\"Origin same/diff features\")\n",
        "print(\"origin features\")\n",
        "merged_data[\"origin_current_diff_published\"] = (\n",
        "            merged_data[\"published_time\"] - merged_data[\"origin_published_time\"]).dt.total_seconds()\n",
        "merged_data[\"origin_current_diff_impression_published\"] = (\n",
        "            merged_data[\"impression_time\"] - merged_data[\"origin_published_time\"]).dt.total_seconds()\n",
        "\n",
        "merged_data[\"origin_current_same_cluster\"] = merged_data[\"origin_cluster\"] == merged_data[\"cluster\"]\n",
        "merged_data[\"origin_current_same_category\"] = merged_data[\"origin_category\"] == merged_data[\"category\"]\n",
        "merged_data[\"origin_current_same_sentiment_label\"] = merged_data[\"origin_sentiment_label\"] == merged_data[\"sentiment_label\"]\n",
        "merged_data[\"origin_current_diff_sentiment_score\"] = merged_data[\"origin_sentiment_score\"] - merged_data[\"sentiment_score\"]\n",
        "\n",
        "def categorize_time_of_day(hour):\n",
        "    if hour < 6:\n",
        "        return 'Night'\n",
        "    elif hour < 12:\n",
        "        return 'Morning'\n",
        "    elif hour < 18:\n",
        "        return 'Afternoon'\n",
        "    else:\n",
        "        return 'Evening'\n",
        "\n",
        "\n",
        "merged_data['time_of_day'] = merged_data['impression_time'].dt.hour.apply(categorize_time_of_day)\n",
        "\n",
        "# Select and rename columns\n",
        "\n",
        "target = [\"clicked\"]\n",
        "ids = [\"impression_id\", \"session_id\", \"user_id\", \"article_id\"]\n",
        "\n",
        "feature_article = ['premium', 'sentiment_score', 'sentiment_label',  'cluster']\n",
        "\n",
        "feature_user = ['user_average_read_time', 'user_average_scroll_percentage',\n",
        "                'user_impression_frequency', 'user_interaction_score', 'user_mood']\n",
        "\n",
        "feature_impression = ['device_type', 'is_sso_user', 'is_subscriber', 'origin_read_time',\n",
        "                      'origin_scroll_percentage', 'coming_from_home_page',\n",
        "                      'origin_sentiment_label', 'origin_sentiment_score',\n",
        "                      'origin_current_diff_published',\n",
        "                      'origin_current_diff_impression_published', 'time_of_day', 'origin_cluster',\n",
        "                      'origin_current_same_cluster','origin_current_same_category',\n",
        "                      'origin_current_same_sentiment_label','origin_current_diff_sentiment_score', 'user_article_same_mood',\n",
        "                   'user_article_favorite',\n",
        "                   'user_article_least_favorite',]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "val_date = '2023-05-23 07:00:00'\n",
        "test_date= '2023-05-24 07:00:00'\n",
        "# Filter train/test\n",
        "merged_data_train = merged_data[merged_data[\"impression_time\"] < val_date]\n",
        "merged_data_val = merged_data[(merged_data[\"impression_time\"] > val_date) & (merged_data[\"impression_time\"] < test_date)]\n",
        "merged_data_test = merged_data[merged_data[\"impression_time\"] > test_date]\n",
        "\n",
        "# Displaying impression time range for each dataset\n",
        "print(f\"\\nTraining data time range: {merged_data_train['impression_time'].min()} to {merged_data_train['impression_time'].max()}\")\n",
        "print(f\"Validation data time range: {merged_data_val['impression_time'].min()} to {merged_data_val['impression_time'].max()}\")\n",
        "print(f\"Test data time range: {merged_data_test['impression_time'].min()} to {merged_data_test['impression_time'].max()}\")\n",
        "\n",
        "final_columns = ids + target + feature_user + feature_article + feature_impression\n",
        "\n",
        "final_data_train = merged_data_train[final_columns]\n",
        "final_data_val = merged_data_val[final_columns]\n",
        "final_data_test = merged_data_test[final_columns]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Validation set\n",
        "print(\"saving to parquet\")\n",
        "\n",
        "file_name_train = f\"train_dataset_{dataset_type}.parquet\"\n",
        "final_data_train.to_parquet(file_name_train, index=False)\n",
        "print(f\"Saved {file_name_train} of length: {len(final_data_train)}\")\n",
        "\n",
        "file_name_val = f\"val_dataset_{dataset_type}.parquet\"\n",
        "final_data_val.to_parquet(file_name_val, index=False)\n",
        "print(f\"Saved {file_name_val} of length: {len(final_data_val)}\")\n",
        "\n",
        "file_name_test = f\"test_dataset_{dataset_type}.parquet\"\n",
        "final_data_test.to_parquet(file_name_test, index=False)\n",
        "print(f\"Saved {file_name_test} of length: {len(final_data_test)}\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Took: {end - start:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# Displaying dataset sizes\n",
        "print(f\"Training data size: {len(final_data_train):,} samples\")\n",
        "print(f\"Validation data size: {len(final_data_val):,} samples\")\n",
        "print(f\"Test data size: {len(final_data_test):,} samples\")\n",
        "\n"
      ]
    }
  ]
}