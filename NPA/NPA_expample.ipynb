{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-04T06:26:45.780063Z",
     "start_time": "2024-11-04T06:26:45.677328Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "NPA [1] is a news recommendation model with personalized attention. \n",
    "The core of NPA is a news representation model and a user representation model. \n",
    "In the news representation model we use a CNN network to learn hidden representations of news articles based on their titles. \n",
    "In the user representation model we learn the representations of users based on the representations of their clicked news articles.\n",
    "In addition, a word-level and a news-level personalized attention are used to capture different informativeness for different users.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "StandardScaler in Scikit-Learn removes the mean and scales the data to unit variance,\n",
    " which is standard practice for continuous data in many machine learning models.\n",
    " \n",
    "OneHotEncoder is used for categorical data to create a binary column for\n",
    " each category and is especially useful for non-ordinal categories.\n",
    " \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "filePath_with_all_features = r\"C:\\Users\\Michalina\\bt4222\\project\\BT4222-project\\xgboost_dataset_ebnerd_demo.parquet\"\n",
    "data = pd.read_parquet(filePath_with_all_features)\n",
    "\n",
    "\n",
    "\n",
    "features_cont = ['read_time', \n",
    "                 'sentiment_score', 'user_average_read_time',\n",
    "                 'user_average_scroll_percentage', 'user_impression_frequency',\n",
    "                 'interaction_score']\n",
    "\n",
    "features_cat = ['sentiment_label', 'user_mood','device_type',\n",
    "                'is_sso_user', 'is_subscriber', 'premium',\n",
    "                'category_encoded', 'subcategory_encoded',\n",
    "                'favorite_category_encoded', 'least_favorite_category_encoded',\n",
    "                'user_article_same_mood']\n",
    "\n",
    "\n",
    "\n",
    "# Normalizing continuous features\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Encoding categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', continuous_transformer, features_cont),\n",
    "        ('cat', categorical_transformer, features_cat)\n",
    "    ])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Get feature names after one-hot encoding for categorical data\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "                     .get_feature_names(input_features=features_cat))\n",
    "feature_names = feature_names + features_cont  # Add continuous features' names\n",
    "\n",
    "# Convert the numpy array returned by ColumnTransformer back to a DataFrame\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "X_processed_df.to_csv('processed_data_for_dl.csv', index=False)\n",
    "\n",
    "\n",
    "#embeddings (11777, 17) where one dim of 17 is unique_id of articles\n",
    "###############################################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#TODO: ADJUSTING A DATALOADER FOR THE LATEST VERSION OF OUR DATA \n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mNPA [1] is a news recommendation model with personalized attention. \u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mThe core of NPA is a news representation model and a user representation model. \u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03mIn addition, a word-level and a news-level personalized attention are used to capture different informativeness for different users.\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03mStandardScaler in Scikit-Learn removes the mean and scales the data to unit variance,\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m which is standard practice for continuous data in many machine learning models.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m \u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler, OneHotEncoder\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompose\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ColumnTransformer\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\assignment3\\lib\\site-packages\\pandas\\__init__.py:57\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;66;03m# numpy compat\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     58\u001B[0m         is_numpy_dev \u001B[38;5;28;01mas\u001B[39;00m _is_numpy_dev,  \u001B[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001B[39;00m\n\u001B[0;32m     59\u001B[0m     )\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m _err:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m     61\u001B[0m     _module \u001B[38;5;241m=\u001B[39m _err\u001B[38;5;241m.\u001B[39mname\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\assignment3\\lib\\site-packages\\pandas\\compat\\__init__.py:26\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_constants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     18\u001B[0m     IS64,\n\u001B[0;32m     19\u001B[0m     ISMUSL,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m     PYPY,\n\u001B[0;32m     24\u001B[0m )\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompressors\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_numpy_dev\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     pa_version_under10p1,\n\u001B[0;32m     29\u001B[0m     pa_version_under11p0,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     34\u001B[0m     pa_version_under17p0,\n\u001B[0;32m     35\u001B[0m )\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\assignment3\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:9\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Version\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# numpy versioning\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m _np_version \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__version__\u001B[49m\n\u001B[0;32m     10\u001B[0m _nlv \u001B[38;5;241m=\u001B[39m Version(_np_version)\n\u001B[0;32m     11\u001B[0m np_version_lt1p23 \u001B[38;5;241m=\u001B[39m _nlv \u001B[38;5;241m<\u001B[39m Version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.23\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'numpy' has no attribute '__version__'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Data Structure in Batches\n",
    "   -------------------------\n",
    "\n",
    "To ensure that the model processes each session separately and provides separate rankings for each session, you need to carefully manage how data is batched:\n",
    "\n",
    "Session Batch Processing:\n",
    "=========================\n",
    "> Each batch should contain multiple sessions, and within each session, \n",
    "you several articles.\n",
    " \n",
    "Feature Organization:\n",
    "=====================\n",
    "\n",
    "For each session in a batch, you'll typically have:\n",
    "- A matrix of article embeddings and features (one row per article).\n",
    "- A single vector of dynamic features specific to that session.\n",
    "- A single vector of static user features (which might be replicated across multiple sessions if they belong to the same user).\n",
    "\n",
    "\n",
    "2. Model Input Handling\n",
    "   --------------------\n",
    "When processing each batch, your model needs to be aware of the boundaries of each session. This can be achieved by:\n",
    "\n",
    "Padding and Masking: \n",
    "====================\n",
    "\n",
    "> If sessions have different numbers of articles:\n",
    " -> we need to pad the sessions to have the same shape and use masking to ignore padded values during attention and subsequent calculations\n",
    " \n",
    "Separate Processing:\n",
    "====================\n",
    "\n",
    "> We need to ensure that your model processes each session independently,\n",
    " -> especially during the attention mechanism, so that the article rankings are computed within the context of each session only\n",
    " \n",
    "\n"
   ],
   "id": "28564456060debd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from Helpers.helpers import split_by_session_train_val_test\n",
    "from NPA.SessionDataset import SessionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data, val_data, test_data  = split_by_session_train_val_test(X_processed_df,test_size=0.2, val_size=0.1)\n",
    "embeddings=pd.read_parquet(r'C:\\Users\\Michalina\\bt4222\\project\\BT4222-project\\ebnerd_demo\\reduced_embeddings.parquet')\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = SessionDataset(train_data,embeddings)\n",
    "val_dataset = SessionDataset(val_data, embeddings)\n",
    "test_dataset = SessionDataset(test_data,embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Since each session can have a varying number of articles, \n",
    "DataLoader's collate function needs to handle batches \n",
    "where each element (session) could have different shapes or sizes.\n",
    "\n",
    "DataLoader can effectively handle batches\n",
    "that contain multiple sessions, each with a varying number of articles\n",
    "\n",
    "\n",
    "Padding in order to have the same  length of the sesssion id articles :\n",
    "\n",
    "Masking is a technique used to prevent the model from considering padded data points during training. It involves creating a mask that specifies which elements in the input data are actual data and which are padding.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    max_articles = max(len(item['articles_features']) for item in batch)\n",
    "    batch_articles_features = []\n",
    "    batch_articles_embeddings = []\n",
    "    batch_user_dynamic_features = []\n",
    "    batch_user_static_features = []\n",
    "    batch_targets = []\n",
    "    masks = []  # To store the mask for targets\n",
    "\n",
    "    for item in batch:\n",
    "        num_articles = len(item['articles_features'])\n",
    "\n",
    "        # Pad features and embeddings\n",
    "        padded_features = torch.nn.functional.pad(item['articles_features'], (0, 0, 0, max_articles - num_articles))\n",
    "        padded_embeddings = torch.nn.functional.pad(item['articles_embeddings'], (0, 0, 0, max_articles - num_articles))\n",
    "\n",
    "        batch_articles_features.append(padded_features)\n",
    "        batch_articles_embeddings.append(padded_embeddings)\n",
    "        \n",
    "        # User features do not need padding\n",
    "        batch_user_dynamic_features.append(item['user_dynamic_features'])\n",
    "        batch_user_static_features.append(item['user_static_features'])\n",
    "\n",
    "        # Handle targets and create mask\n",
    "        target = item['target']\n",
    "        mask = torch.ones(num_articles, dtype=torch.bool)  # True for actual data\n",
    "        if num_articles < max_articles:\n",
    "            padding = max_articles - num_articles\n",
    "            padded_target = torch.nn.functional.pad(target, (0, padding), value=-1)  # Pad target with an invalid class or value\n",
    "            padded_mask = torch.nn.functional.pad(mask, (0, padding), value=False)  # False for padded data\n",
    "        else:\n",
    "            padded_target = target\n",
    "            padded_mask = mask\n",
    "\n",
    "        batch_targets.append(padded_target)\n",
    "        masks.append(padded_mask)\n",
    "\n",
    "    return {\n",
    "        'articles_features': torch.stack(batch_articles_features),\n",
    "        'articles_embeddings': torch.stack(batch_articles_embeddings),\n",
    "        'user_dynamic_features': torch.stack(batch_user_dynamic_features),\n",
    "        'user_static_features': torch.stack(batch_user_static_features),\n",
    "        'target': torch.stack(batch_targets),\n",
    "        'mask': torch.stack(masks)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "#Params \n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "shuffle_train = True \n",
    "shuffle_val_test = False\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train, \n",
    "                          num_workers=num_workers, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Create DataLoader for validation data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle_val_test, \n",
    "                        num_workers=num_workers, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Create DataLoader for test data\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle_val_test, \n",
    "                         num_workers=num_workers, collate_fn=custom_collate_fn)\n"
   ],
   "id": "f46f3ece610d3139"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model training",
   "id": "831bbbc233f84ae8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from NPA.PairwiseRankingLoss import PairwiseRankingLoss\n",
    "from torch import optim, nn\n",
    "from NPA.NPAModel import NPAModel\n",
    "\n",
    "article_features= ['category_encoded', 'subcategory_encoded','sentiment_score','sentiment_label','premium']\n",
    "\n",
    "\n",
    "user_dynamic_features_col = ['device_type']\n",
    "\n",
    "user_static_features_col =['user_mood',\n",
    "                'is_sso_user', 'is_subscriber',\n",
    "                'favorite_category_encoded', 'least_favorite_category_encoded',\n",
    "                'user_article_same_mood','read_time', 'user_average_read_time',\n",
    "                 'user_average_scroll_percentage', 'user_impression_frequency',\n",
    "                 'interaction_score']\n",
    "\n",
    "model = NPAModel(\n",
    "    embedding_dim=16,\n",
    "    num_filters=32,\n",
    "    kernel_size=3,\n",
    "    dynamic_feature_dim=len(user_dynamic_features_col),\n",
    "    static_feature_dim=len(user_static_features_col),\n",
    "    additional_article_features_dim=len(article_features)\n",
    "    )\n",
    "\n",
    "# Assuming model is already created and available\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust learning rate as needed\n",
    "\n",
    "# Define the loss function\n",
    "criterion = PairwiseRankingLoss(margin=1.0)\n",
    "\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        embeddings = data['articles_embeddings']\n",
    "        features = data['articles_features']\n",
    "        user_features = data['user_dynamic_features']\n",
    "        static_features = data['user_static_features']\n",
    "        targets = data['target']\n",
    "        mask = data['mask']  # Make sure mask is correctly computed in DataLoader\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(embeddings, features, user_features, static_features)\n",
    "        outputs = outputs.squeeze()  # Ensure output dimensions are correct\n",
    "\n",
    "        loss = criterion(outputs, targets, mask)  # Pass the mask to the loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Assuming loss is already a scalar\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation and testing steps should also apply masks if using padded data\n"
   ],
   "id": "dd022b2e0ebc8c83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
