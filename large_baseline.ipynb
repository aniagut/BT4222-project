{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "F2FiqAaDBw1T"
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, Dense, Flatten, TextVectorization, Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import mixed_precision, layers, metrics, backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ebnerd_type = \"ebnerd_large\""
   ],
   "metadata": {
    "id": "D5V4eDH4CXbv"
   },
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load datasets\n",
    "behaviors = pd.read_parquet(f\"{ebnerd_type}/train/behaviors.parquet\").dropna()\n",
    "history = pd.read_parquet(f\"{ebnerd_type}/train/history.parquet\").dropna()\n",
    "articles = pd.read_parquet(f\"{ebnerd_type}/articles.parquet\").dropna()\n",
    "articles_embeddings = pd.read_parquet(f\"{ebnerd_type}/bert_base_multilingual_cased.parquet\")"
   ],
   "metadata": {
    "id": "NNOK8_m0CnlD"
   },
   "execution_count": 111,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "behaviors['article_id'] = behaviors['article_id'].astype(int)"
   ],
   "metadata": {
    "id": "gy_cWxNfGRGE"
   },
   "execution_count": 112,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_behaviors = behaviors[behaviors['impression_time'] < '2023-05-23 07:00:00']\n",
    "val_behaviors = behaviors[(behaviors['impression_time'] >= '2023-05-23 07:00:00') &\n",
    "                        (behaviors['impression_time'] < '2023-05-24 07:00:00')]\n",
    "test_behaviors = behaviors[behaviors['impression_time'] >= '2023-05-24 07:00:00']"
   ],
   "metadata": {
    "id": "TAo6LRp_Mjkx"
   },
   "execution_count": 113,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# generate user-item interactions with positive and negative samples\n",
    "test_user_item_pairs = []\n",
    "val_user_item_pairs = []\n",
    "train_user_item_pairs = []\n",
    "unique_users = behaviors['user_id'].unique()\n",
    "unique_items = articles['article_id'].unique()\n",
    "\n",
    "for user_id in unique_users:\n",
    "    # get all sessions for this user\n",
    "    user_data = train_behaviors[train_behaviors['user_id'] == user_id]\n",
    "\n",
    "    # iterate over each session to gather clicked and non-clicked inview articles\n",
    "    for _, session in user_data.iterrows():\n",
    "        # extract clicked and inview article IDs for the session\n",
    "        clicked_articles = set(session['article_ids_clicked']) if len(session['article_ids_clicked']) > 0 else set()\n",
    "        inview_articles = set(session['article_ids_inview']) if len(session['article_ids_inview']) > 0 else set()\n",
    "\n",
    "        impression_id = session['impression_id']\n",
    "        impression_time = session['impression_time']\n",
    "\n",
    "        # add positive samples (clicked articles)\n",
    "        for article_id in clicked_articles:\n",
    "            train_user_item_pairs.append([user_id, article_id, impression_id, impression_time, 1])  # Interaction = 1 for positive samples\n",
    "\n",
    "        # identify non-clicked inview articles as additional negative samples\n",
    "        non_clicked_inview = list(inview_articles - clicked_articles)\n",
    "\n",
    "        non_clicked_inview = random.sample(non_clicked_inview, min(3 * len(clicked_articles), len(non_clicked_inview)))\n",
    "\n",
    "        for article_id in non_clicked_inview:\n",
    "            train_user_item_pairs.append([user_id, article_id, impression_id, impression_time, 0])  # interaction = 0 for non-clicked inview articles\n",
    "\n",
    "for user_id in unique_users:\n",
    "    # get all sessions for this user\n",
    "    user_data = val_behaviors[val_behaviors['user_id'] == user_id]\n",
    "\n",
    "    # iterate over each session to gather clicked and non-clicked inview articles\n",
    "    for _, session in user_data.iterrows():\n",
    "        # extract clicked and inview article IDs for the session\n",
    "        clicked_articles = set(session['article_ids_clicked']) if len(session['article_ids_clicked']) > 0 else set()\n",
    "        inview_articles = set(session['article_ids_inview']) if len(session['article_ids_inview']) > 0 else set()\n",
    "\n",
    "        impression_id = session['impression_id']\n",
    "        impression_time = session['impression_time']\n",
    "\n",
    "        # add positive samples (clicked articles)\n",
    "        for article_id in clicked_articles:\n",
    "            val_user_item_pairs.append([user_id, article_id, impression_id, impression_time, 1])  # Interaction = 1 for positive samples\n",
    "\n",
    "        # identify non-clicked inview articles as additional negative samples\n",
    "        non_clicked_inview = random.sample(non_clicked_inview, min(3 * len(clicked_articles), len(non_clicked_inview)))\n",
    "\n",
    "\n",
    "        for article_id in non_clicked_inview:\n",
    "            val_user_item_pairs.append([user_id, article_id, impression_id, impression_time, 0])  # interaction = 0 for non-clicked inview articles\n",
    "\n",
    "for user_id in unique_users:\n",
    "    # get all sessions for this user\n",
    "    user_data = test_behaviors[test_behaviors['user_id'] == user_id]\n",
    "\n",
    "    # iterate over each session to gather clicked and non-clicked inview articles\n",
    "    for _, session in user_data.iterrows():\n",
    "        # extract clicked and inview article IDs for the session\n",
    "        clicked_articles = set(session['article_ids_clicked']) if len(session['article_ids_clicked']) > 0 else set()\n",
    "        inview_articles = set(session['article_ids_inview']) if len(session['article_ids_inview']) > 0 else set()\n",
    "\n",
    "        impression_id = session['impression_id']\n",
    "        impression_time = session['impression_time']\n",
    "\n",
    "        # add positive samples (clicked articles)\n",
    "        for article_id in clicked_articles:\n",
    "            test_user_item_pairs.append([user_id, article_id, impression_id, impression_time, 1])  # Interaction = 1 for positive samples\n",
    "\n",
    "        # identify non-clicked inview articles as additional negative samples\n",
    "        non_clicked_inview = list(inview_articles - clicked_articles)\n",
    "        non_clicked_inview = random.sample(non_clicked_inview, min(3 * len(clicked_articles), len(non_clicked_inview)))\n",
    "\n",
    "        for article_id in non_clicked_inview:\n",
    "            test_user_item_pairs.append([user_id, article_id, impression_id, impression_time, 0])  # interaction = 0 for non-clicked inview articles"
   ],
   "metadata": {
    "id": "-rR0B0U_Et_Y"
   },
   "execution_count": 114,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# convert to DataFrame\n",
    "train_interactions_df = pd.DataFrame(train_user_item_pairs, columns=['user_id', 'article_id', 'impression_id', 'impression_time', 'interaction'])\n",
    "val_interactions_df = pd.DataFrame(val_user_item_pairs, columns=['user_id', 'article_id', 'impression_id', 'impression_time', 'interaction'])\n",
    "test_interactions_df = pd.DataFrame(test_user_item_pairs, columns=['user_id', 'article_id', 'impression_id', 'impression_time', 'interaction'])"
   ],
   "metadata": {
    "id": "kZqBfCOxddcR"
   },
   "execution_count": 115,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_interactions_df = pd.merge(\n",
    "    train_interactions_df,\n",
    "    articles[['article_id', 'category', 'premium']],\n",
    "    on='article_id',\n",
    "    how='left'\n",
    ")\n",
    "val_interactions_df = pd.merge(\n",
    "    val_interactions_df,\n",
    "    articles[['article_id', 'category', 'premium']],\n",
    "    on='article_id',\n",
    "    how='left'\n",
    ")\n",
    "test_interactions_df = pd.merge(\n",
    "    test_interactions_df,\n",
    "    articles[['article_id', 'category', 'premium']],\n",
    "    on='article_id',\n",
    "    how='left'\n",
    ")"
   ],
   "metadata": {
    "id": "yrlmh1syG0vv"
   },
   "execution_count": 116,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_interactions_df['category'] = train_interactions_df['category'].fillna(-1).astype(int)\n",
    "val_interactions_df['category'] = val_interactions_df['category'].fillna(-1).astype(int)\n",
    "test_interactions_df['category'] = test_interactions_df['category'].fillna(-1).astype(int)"
   ],
   "metadata": {
    "id": "dm20cyEoNtUt"
   },
   "execution_count": 117,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history_features = history.groupby('user_id').agg({\n",
    "    'read_time_fixed': lambda x: np.nanmean([t for t in x if t is not None]),  # average read time\n",
    "    'scroll_percentage_fixed': lambda x: np.nanmean([s for s in x if s is not None]),  # average scroll percentage\n",
    "    'article_id_fixed': 'count'  # total clicks\n",
    "}).reset_index()\n",
    "\n",
    "history_features.columns = ['user_id', 'avg_read_time', 'avg_scroll_percentage', 'total_clicks']"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ym29L-MNIhGl",
    "outputId": "265bec32-77f8-4233-c938-eaa33794d2e1"
   },
   "execution_count": 118,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_interactions_df = pd.merge(\n",
    "    train_interactions_df,\n",
    "    history_features[['user_id', 'avg_read_time', 'avg_scroll_percentage', 'total_clicks']],\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")\n",
    "val_interactions_df = pd.merge(\n",
    "    val_interactions_df,\n",
    "    history_features[['user_id', 'avg_read_time', 'avg_scroll_percentage', 'total_clicks']],\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")\n",
    "test_interactions_df = pd.merge(\n",
    "    test_interactions_df,\n",
    "    history_features[['user_id', 'avg_read_time', 'avg_scroll_percentage', 'total_clicks']],\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")"
   ],
   "metadata": {
    "id": "ZbDPK2_mJU9v"
   },
   "execution_count": 119,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# fav category\n",
    "article_category_map = dict(zip(articles['article_id'], articles['category']))"
   ],
   "metadata": {
    "id": "RzFsVU7wJ-i_"
   },
   "execution_count": 120,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_favorite_category(row):\n",
    "    \"\"\"\n",
    "    Determine the favorite category for a user based on the articles they have read.\n",
    "    \"\"\"\n",
    "    user_id = row['user_id']\n",
    "    article_ids = row['article_id_fixed']\n",
    "\n",
    "    # map article IDs to categories using the dictionary\n",
    "    categories = [article_category_map.get(article_id) for article_id in article_ids if article_id in article_category_map]\n",
    "\n",
    "    # count the frequency of each category and get the most common one\n",
    "    if categories:\n",
    "        category_count = Counter(categories)\n",
    "        favorite_category = category_count.most_common(1)[0][0]  # get the most common category\n",
    "    else:\n",
    "        favorite_category = 'Unknown'\n",
    "\n",
    "    return pd.Series({'user_id': user_id, 'favorite_category': favorite_category})"
   ],
   "metadata": {
    "id": "Re9g1UpNHN0s"
   },
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "user_favorite_categories = history.apply(get_favorite_category, axis=1)"
   ],
   "metadata": {
    "id": "2yMkAMXHHf-N"
   },
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_interactions_df = pd.merge(\n",
    "    train_interactions_df,\n",
    "    user_favorite_categories,\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")\n",
    "val_interactions_df = pd.merge(\n",
    "    val_interactions_df,\n",
    "    user_favorite_categories,\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")\n",
    "test_interactions_df = pd.merge(\n",
    "    test_interactions_df,\n",
    "    user_favorite_categories,\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")"
   ],
   "metadata": {
    "id": "0IVaodxlHwpO"
   },
   "execution_count": 123,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_interactions_df['favorite_category'].fillna(-1, inplace=True)\n",
    "val_interactions_df['favorite_category'].fillna(-1, inplace=True)\n",
    "test_interactions_df['favorite_category'].fillna(-1, inplace=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wF-WMYFSH-Hm",
    "outputId": "43a108f0-9e6d-4ce3-8580-491ca4be67ea"
   },
   "execution_count": 124,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# user info\n",
    "user_info = behaviors[['user_id', 'gender', 'postcode', 'age', 'is_subscriber',\n",
    "                            'next_read_time', 'next_scroll_percentage']].sort_values(by='next_read_time', ascending=False).drop_duplicates(subset=['user_id'])"
   ],
   "metadata": {
    "id": "EllWKlGEIMQr"
   },
   "execution_count": 125,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_interactions_df = pd.merge(\n",
    "    train_interactions_df,\n",
    "    user_info,\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")\n",
    "val_interactions_df = pd.merge(\n",
    "    val_interactions_df,\n",
    "    user_info,\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")\n",
    "test_interactions_df = pd.merge(\n",
    "    test_interactions_df,\n",
    "    user_info,\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")"
   ],
   "metadata": {
    "id": "-YnS--mdIUbe"
   },
   "execution_count": 126,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save to parquet - no embeddings\n",
    "train_interactions_df.to_parquet(f\"interactions_train.parquet\")\n",
    "val_interactions_df.to_parquet(f\"interactions_val.parquet\")\n",
    "test_interactions_df.to_parquet(f\"interactions_test.parquet\")"
   ],
   "metadata": {
    "id": "4W6tKxGGKQ7P"
   },
   "execution_count": 83,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add embeddings\n",
    "train_interactions_df = pd.merge(\n",
    "    train_interactions_df,\n",
    "    articles_embeddings,\n",
    "    on='article_id',\n",
    "    how='left'\n",
    ")\n",
    "val_interactions_df = pd.merge(\n",
    "    val_interactions_df,\n",
    "    articles_embeddings,\n",
    "    on='article_id',\n",
    "    how='left'\n",
    ")\n",
    "test_interactions_df = pd.merge(\n",
    "    test_interactions_df,\n",
    "    articles_embeddings,\n",
    "    on='article_id',\n",
    "    how='left'\n",
    ")"
   ],
   "metadata": {
    "id": "m74iTGuQhHRA"
   },
   "execution_count": 127,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save to parquet - embeddings\n",
    "train_interactions_df.to_parquet(f\"interactions_train_embeddings.parquet\")\n",
    "val_interactions_df.to_parquet(f\"interactions_val_embeddings.parquet\")\n",
    "test_interactions_df.to_parquet(f\"interactions_test_embeddings.parquet\")"
   ],
   "metadata": {
    "id": "KR_W3ZlThWBg"
   },
   "execution_count": 128,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_interactions_df = pd.read_parquet(f\"interactions_train_embeddings.parquet\")\n",
    "val_interactions_df = pd.read_parquet(f\"interactions_val_embeddings.parquet\")\n",
    "test_interactions_df = pd.read_parquet(f\"interactions_test_embeddings.parquet\")"
   ],
   "metadata": {
    "id": "JWfopdshRJD6"
   },
   "execution_count": 86,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# split into test, train, validation\n",
    "train_df = train_interactions_df\n",
    "val_df = val_interactions_df\n",
    "test_df = test_interactions_df\n",
    "print(f\"Train set: {len(train_df)}, Validation set: {len(val_df)}, Test set: {len(test_df)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axKgzJCBHs38",
    "outputId": "a11f12e3-b102-4ce3-9492-982cd8065086"
   },
   "execution_count": 129,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "unique_users = behaviors['user_id'].unique()\n",
    "unique_articles = behaviors['article_id'].unique()\n",
    "unique_categories = articles['category'].unique()"
   ],
   "metadata": {
    "id": "Wcg3SE1-MwOG"
   },
   "execution_count": 130,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "categorical_features = ['user_id', 'article_id', 'category', 'favorite_category', 'gender', 'postcode', 'is_subscriber']"
   ],
   "metadata": {
    "id": "QKPGMAx-h_J9"
   },
   "execution_count": 131,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoders = {}\n",
    "for feature in categorical_features:\n",
    "    encoder = LabelEncoder()\n",
    "    train_df[feature] = encoder.fit_transform(train_df[feature])\n",
    "    val_df[feature] = encoder.fit_transform(val_df[feature])\n",
    "    test_df[feature] = encoder.fit_transform(test_df[feature])\n",
    "    encoders[feature] = encoder"
   ],
   "metadata": {
    "id": "mhVk00zoiD-y"
   },
   "execution_count": 132,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# normalize numerical features\n",
    "numerical_features = ['avg_read_time', 'avg_scroll_percentage', 'total_clicks', 'age', 'next_read_time', 'next_scroll_percentage']\n",
    "scaler = StandardScaler()\n",
    "train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\n",
    "val_df[numerical_features] = scaler.transform(val_df[numerical_features])\n",
    "test_df[numerical_features] = scaler.transform(test_df[numerical_features])"
   ],
   "metadata": {
    "id": "t5LiYxh6X9ir"
   },
   "execution_count": 133,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# extract BERT embeddings into a numpy array\n",
    "train_df['bert_embeddings'] = train_df['google-bert/bert-base-multilingual-cased'].apply(lambda x: np.array(x))\n",
    "train_df_bert_embeddings = np.stack(train_df['bert_embeddings'].values)\n",
    "\n",
    "val_df['bert_embeddings'] = val_df['google-bert/bert-base-multilingual-cased'].apply(lambda x: np.array(x))\n",
    "val_df_bert_embeddings = np.stack(val_df['bert_embeddings'].values)\n",
    "\n",
    "test_df['bert_embeddings'] = test_df['google-bert/bert-base-multilingual-cased'].apply(lambda x: np.array(x))\n",
    "test_df_bert_embeddings = np.stack(test_df['bert_embeddings'].values)"
   ],
   "metadata": {
    "id": "E8JhL0f_jIiP"
   },
   "execution_count": 134,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_inputs(df):\n",
    "    categorical_inputs = df[categorical_features].values\n",
    "    numerical_inputs = df[numerical_features].values\n",
    "    bert_embeddings = np.stack(df['bert_embeddings'].values)\n",
    "    return [categorical_inputs, numerical_inputs, bert_embeddings]"
   ],
   "metadata": {
    "id": "zJM5T4-JjtMi"
   },
   "execution_count": 135,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_inputs = prepare_inputs(train_df)\n",
    "val_inputs = prepare_inputs(val_df)\n",
    "test_inputs = prepare_inputs(test_df)\n",
    "\n",
    "train_labels = train_df['interaction'].values\n",
    "val_labels = val_df['interaction'].values\n",
    "test_labels = test_df['interaction'].values"
   ],
   "metadata": {
    "id": "us0SCtVijuoq"
   },
   "execution_count": 136,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define feature input layers\n",
    "categorical_input = layers.Input(shape=(len(categorical_features),), name='categorical_input')\n",
    "numerical_input = layers.Input(shape=(len(numerical_features),), name='numerical_input')\n",
    "bert_input = layers.Input(shape=(train_df_bert_embeddings.shape[1],), name='bert_input')"
   ],
   "metadata": {
    "id": "N3KsFlUQOvvO"
   },
   "execution_count": 137,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# embedding and dense layers for categorical features\n",
    "x1 = layers.Embedding(input_dim=len(unique_users), output_dim=32)(categorical_input)\n",
    "x1 = layers.Flatten()(x1)\n",
    "\n",
    "# dense layers for numerical features\n",
    "x2 = layers.Dense(32, activation='relu')(numerical_input)\n",
    "\n",
    "# use BERT embeddings directly\n",
    "x3 = layers.Dense(128, activation='relu')(bert_input)\n",
    "\n",
    "# concatenate all features\n",
    "x = layers.Concatenate()([x1, x2, x3])\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(x)"
   ],
   "metadata": {
    "id": "EXcwC7fvkC47"
   },
   "execution_count": 138,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define the model\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        fl = -alpha_t * K.pow((1 - p_t), gamma) * K.log(p_t)\n",
    "        return K.mean(fl)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "model = Model(inputs=[categorical_input, numerical_input, bert_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2, alpha=0.25), metrics=[metrics.AUC()])\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "ILz9YwnqPQC0",
    "outputId": "c5eb3e52-c892-4ccd-ef85-1cac08dd80d9"
   },
   "execution_count": 144,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# fine tuning\n",
    "# lr scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_auc_3',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}"
   ],
   "metadata": {
    "id": "xrhgBbUAuM-O"
   },
   "execution_count": 147,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc_3',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    [train_inputs[0], train_inputs[1], train_inputs[2]], train_labels,\n",
    "    validation_data=([val_inputs[0], val_inputs[1], val_inputs[2]], val_labels),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMF8QlpUPgml",
    "outputId": "eeee12d8-45bd-4365-d00d-123d3f277272"
   },
   "execution_count": 148,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# predict on the test set\n",
    "test_preds = model.predict([test_inputs[0], test_inputs[1], test_inputs[2]]).flatten()\n",
    "test_df['predicted_score'] = test_preds"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qASvjKdYPmCy",
    "outputId": "40bcf2e1-07ce-4ada-8387-e485d1e02881"
   },
   "execution_count": 149,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(test_df[['impression_id', 'user_id', 'article_id','interaction', 'predicted_score']])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GT0bEoLInBnG",
    "outputId": "b99eb481-cbc5-43c1-ad8b-58abb706bda7"
   },
   "execution_count": 150,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#metrics\n",
    "# AUC\n",
    "def calculate_auc(data):\n",
    "    \"\"\"Calculate AUC for a given dataset.\"\"\"\n",
    "    if data['interaction'].nunique() < 2:  # AUC needs at least one positive and one negative label\n",
    "        return np.nan\n",
    "    return roc_auc_score(data['interaction'], data['predicted_score'])\n",
    "\n",
    "# MRR calculation\n",
    "def calculate_mrr(data):\n",
    "    \"\"\"Calculate MRR for a given dataset.\"\"\"\n",
    "    sorted_data = data.sort_values(by='predicted_score', ascending=False)\n",
    "    ranks = sorted_data['interaction'].values\n",
    "    for rank, interaction in enumerate(ranks, start=1):\n",
    "        if interaction == 1:\n",
    "            return 1 / rank\n",
    "    return 0\n",
    "\n",
    "# NDCG calculation\n",
    "def dcg(scores, k):\n",
    "    \"\"\"Calculate Discounted Cumulative Gain.\"\"\"\n",
    "    return sum([score / np.log2(idx + 2) for idx, score in enumerate(scores[:k])])\n",
    "\n",
    "def calculate_ndcg(data, k):\n",
    "    \"\"\"Calculate NDCG for a given dataset and cutoff k.\"\"\"\n",
    "    sorted_data = data.sort_values(by='predicted_score', ascending=False)\n",
    "    ideal_sorted_data = data.sort_values(by='interaction', ascending=False)\n",
    "    dcg_k = dcg(sorted_data['interaction'].values, k)\n",
    "    idcg_k = dcg(ideal_sorted_data['interaction'].values, k)\n",
    "    return dcg_k / idcg_k if idcg_k > 0 else 0\n"
   ],
   "metadata": {
    "id": "0V7hR5sEmWFR"
   },
   "execution_count": 151,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(test_df):\n",
    "    \"\"\"Evaluate the model using AUC, MRR, and NDCG metrics.\"\"\"\n",
    "    auc = test_df.groupby('impression_id').apply(calculate_auc).mean()\n",
    "    mrr = test_df.groupby('impression_id').apply(calculate_mrr).mean()\n",
    "    ndcg3 = test_df.groupby('impression_id').apply(lambda x: calculate_ndcg(x, k=3)).mean()\n",
    "    ndcg5 = test_df.groupby('impression_id').apply(lambda x: calculate_ndcg(x, k=5)).mean()\n",
    "\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"MRR: {mrr:.4f}\")\n",
    "    print(f\"NDCG@3: {ndcg3:.4f}\")\n",
    "    print(f\"NDCG@5: {ndcg5:.4f}\")"
   ],
   "metadata": {
    "id": "ZkYSw7HzmkCK"
   },
   "execution_count": 152,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluate model\n",
    "evaluate_model(test_df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0CksQCEPv2F",
    "outputId": "868bd506-de60-40fa-cedc-9ba4e06b7534"
   },
   "execution_count": 153,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_by_impression(test_data):\n",
    "    results = []\n",
    "    grouped = test_data.groupby('impression_id')\n",
    "\n",
    "    for impression_id, group in grouped:\n",
    "        auc = calculate_auc(group)\n",
    "        mrr = calculate_mrr(group)\n",
    "        ndcg3 = calculate_ndcg(group, 3)\n",
    "        ndcg5 = calculate_ndcg(group, 5)\n",
    "        ndcg10 = calculate_ndcg(group, 10)\n",
    "\n",
    "        # Append the result as a dictionary\n",
    "        results.append({\n",
    "            'impression_id': impression_id,\n",
    "            'AUC': auc,\n",
    "            'MRR': mrr,\n",
    "            'NDCG@3': ndcg3,\n",
    "            'NDCG@5': ndcg5,\n",
    "            'NDCG@10': ndcg10\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    return pd.DataFrame(results)"
   ],
   "metadata": {
    "id": "KZsB1EqqtENt"
   },
   "execution_count": 154,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results = evaluate_by_impression(test_df)\n",
    "print(results)"
   ],
   "metadata": {
    "id": "Rtfj1WYzQWeY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97f9972f-3e62-422f-cfe1-2ce02f475295"
   },
   "execution_count": 155,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the number of bins for histograms\n",
    "bins = 20\n",
    "\n",
    "# Plot 1: AUC Distribution Across Sessions\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(results['AUC'], kde=True, color=\"skyblue\", bins=bins)\n",
    "plt.title(\"AUC Distribution Across Sessions\")\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: MRR Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(results['MRR'], kde=True, color=\"salmon\", bins=bins)\n",
    "plt.title(\"MRR Distribution Across Sessions\")\n",
    "plt.xlabel(\"MRR\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: NDCG@3 Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(results['NDCG@3'], kde=True, color=\"lightgreen\", bins=bins)\n",
    "plt.title(\"NDCG@3 Distribution Across Sessions\")\n",
    "plt.xlabel(\"NDCG@3 Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: NDCG@5 Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(results['NDCG@5'], kde=True, color=\"orange\", bins=bins)\n",
    "plt.title(\"NDCG@5 Distribution Across Sessions\")\n",
    "plt.xlabel(\"NDCG@5 Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 5: NDCG@10 Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(results['NDCG@10'], kde=True, color=\"purple\", bins=bins)\n",
    "plt.title(\"NDCG@10 Distribution Across Sessions\")\n",
    "plt.xlabel(\"NDCG@10 Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R7yGAQ7otTOx",
    "outputId": "d7a3818a-5bf2-43c4-c11d-b3b0cc7c405f"
   },
   "execution_count": 156,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "e4nG1t3uw_Ez"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
