{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NPA [1] is a news recommendation model with personalized attention. \n",
    "The core of NPA is a news representation model and a user representation model. \n",
    "In the news representation model we use a CNN network to learn hidden representations of news articles based on their titles. \n",
    "In the user representation model we learn the representations of users based on the representations of their clicked news articles.\n",
    "In addition, a word-level and a news-level personalized attention are used to capture different informativeness for different users.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "StandardScaler in Scikit-Learn removes the mean and scales the data to unit variance,\n",
    " which is standard practice for continuous data in many machine learning models.\n",
    " \n",
    "OneHotEncoder is used for categorical data to create a binary column for\n",
    " each category and is especially useful for non-ordinal categories.\n",
    " \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "filepth_all_data_wthout_embeddigs = \".csv\"\n",
    "data = pd.read_csv(filepth_all_data_wthout_embeddigs)\n",
    "\n",
    "\n",
    "features_cont = ['read_time', \n",
    "                 'sentiment_score', 'user_average_read_time',\n",
    "                 'user_average_scroll_percentage', 'user_impression_frequency',\n",
    "                 'interaction_score']\n",
    "\n",
    "features_cat = ['sentiment_label', 'user_mood','device_type',\n",
    "                'is_sso_user', 'is_subscriber', 'premium',\n",
    "                'category_encoded', 'subcategory_encoded',\n",
    "                'favorite_category_encoded', 'least_favorite_category_encoded',\n",
    "                'user_article_same_mood']\n",
    "\n",
    "\n",
    "\n",
    "# Normalizing continuous features\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Encoding categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', continuous_transformer, features_cont),\n",
    "        ('cat', categorical_transformer, features_cat)\n",
    "    ])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Get feature names after one-hot encoding for categorical data\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "                     .get_feature_names(input_features=features_cat))\n",
    "feature_names = feature_names + features_cont  # Add continuous features' names\n",
    "\n",
    "# Convert the numpy array returned by ColumnTransformer back to a DataFrame\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "X_processed_df.to_csv('processed_data_for_dl.csv', index=False)\n",
    "\n",
    "\n",
    "#embeddings (11777, 17) where one dim of 17 is unique_id of articles\n",
    "###############################################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#TODO: ADJUSTING A DATALOADER FOR THE LATEST VERSION OF OUR DATA \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model training",
   "id": "831bbbc233f84ae8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from NPAModel import NPAModel\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, data_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for articles, user_features, article_stats, clicks in data_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(articles, user_features, article_stats)\n",
    "            loss = criterion(outputs.squeeze(), clicks)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print average loss for the epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader)}')"
   ],
   "id": "dd022b2e0ebc8c83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from metrics import calculate_ndcg\n",
    "\n",
    "def predict_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for articles, user_features, article_stats, actual_clicks in data_loader:\n",
    "            outputs = model(articles, user_features, article_stats)\n",
    "            predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "            actuals.extend(actual_clicks.cpu().numpy())\n",
    "    return predictions, actuals\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for articles, user_features, article_stats, clicks in data_loader:\n",
    "            outputs = model(articles, user_features, article_stats)\n",
    "            predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "            actuals.extend(clicks.cpu().numpy())\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'predicted_score': predictions,\n",
    "        'clicked': actuals\n",
    "    })\n",
    "    ndcg_score = calculate_ndcg(results, 'predicted_score', 'clicked', k=10)\n",
    "    return ndcg_score\n",
    "\n"
   ],
   "id": "15ec7dcf9d936335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prepare_data(articles_embeddings, user_features, article_stats, clicks):\n",
    "    articles_embeddings = torch.tensor(articles_embeddings, dtype=torch.float32)\n",
    "    user_features = torch.tensor(user_features, dtype=torch.float32)\n",
    "    article_stats = torch.tensor(article_stats, dtype=torch.float32)\n",
    "    clicks = torch.tensor(clicks, dtype=torch.float32)\n",
    "    return articles_embeddings, user_features, article_stats, clicks"
   ],
   "id": "463db0ecf25dc9b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "#articles_embeddings, user_features, article_stats, clicks i ASUSME WE HAVE THOSE DATA INT THE LATS DATASET \n",
    "articles, users, stats, clicks = prepare_data(articles_embeddings, user_features, article_stats, clicks)\n",
    "# Convert datasets to TensorDataset\n",
    "full_dataset = TensorDataset(articles, users, stats, clicks)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n"
   ],
   "id": "47c84f0594f1ac57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model instantiation\n",
    "# Initialize the model\n",
    "model = NPAModel(num_words=10000, embedding_dim=100, num_filters=128, kernel_size=3, num_user_features=300, num_article_stats=10)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()  # Move model to GPU if CUDA is available\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, optimizer, criterion, num_epochs=10)\n",
    "\n",
    "# Evaluate NDCG on test set\n",
    "ndcg_score = evaluate_model(model, test_loader)\n",
    "print(f\"Test NDCG Score: {ndcg_score}\")\n"
   ],
   "id": "19ec5a5ef7e5627c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
